# -*- coding: utf-8 -*-
"""IVA ASS-5 21MIA1011.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cddjkn_OcHkSE1CVzhvYVuqeSRlCboav
"""

pip install opencv-python

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Load the video
video_path = '/content/videoplayback (1).mp4'  # Change this to your video path
cap = cv2.VideoCapture(video_path)

# Check if the video opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
    exit()

# Initialize variables
prev_frame = None
event_frames = []
event_timestamps = []

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Convert frame to grayscale
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)  # Optional: Blur for noise reduction

    # Process the first frame
    if prev_frame is None:
        prev_frame = gray_frame
        continue

    # Frame differencing
    diff_frame = cv2.absdiff(prev_frame, gray_frame)

    # Thresholding the difference
    _, thresh_frame = cv2.threshold(diff_frame, 30, 255, cv2.THRESH_BINARY)

    # Morphological operations to remove noise
    kernel = np.ones((5, 5), np.uint8)
    thresh_frame = cv2.morphologyEx(thresh_frame, cv2.MORPH_CLOSE, kernel)
    thresh_frame = cv2.morphologyEx(thresh_frame, cv2.MORPH_OPEN, kernel)

    # Find contours of the moving regions
    contours, _ = cv2.findContours(thresh_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    motion_detected = False

    for contour in contours:
        area = cv2.contourArea(contour)
        if area > 500:  # Minimum area threshold to consider it as motion
            (x, y, w, h) = cv2.boundingRect(contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw rectangle around motion
            motion_detected = True

    # Check for events
    if motion_detected:
        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Current timestamp in seconds
        event_frames.append(frame)
        event_timestamps.append(timestamp)

    # Display the frame with detected motion
    cv2_imshow(frame)  # Use cv2_imshow instead of cv2.imshow

    # Update the previous frame
    prev_frame = gray_frame

    # Break the loop on 'q' key press (in this case, we can stop it after showing a few frames)
    if cv2.waitKey(30) & 0xFF == ord('q'):
        break

# Release video capture
cap.release()

# Print detected events with timestamps
for i, timestamp in enumerate(event_timestamps):
    print(f"Event detected at {timestamp:.2f} seconds")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow  # Use cv2_imshow for displaying images in Colab

def detect_faces(image):
    """Detect faces in the image using skin color thresholding."""
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

    # Define skin color range
    lower_skin = np.array([0, 20, 70], dtype=np.uint8)
    upper_skin = np.array([20, 255, 255], dtype=np.uint8)

    # Create a binary mask for skin color
    mask = cv2.inRange(hsv, lower_skin, upper_skin)

    # Find contours in the mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Filter contours based on area
    faces = []
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > 1000:  # Minimum area threshold for a face
            x, y, w, h = cv2.boundingRect(contour)
            faces.append((x, y, w, h))

    return faces

def analyze_expression(face):
    """Analyze the facial expression based on simple geometry."""
    x, y, w, h = face
    smile_ratio = (w / 2) / h  # Simplistic smile ratio

    # Determine sentiment based on mouth curvature
    if smile_ratio > 0.5:
        return 'Happy'
    elif smile_ratio < 0.3:
        return 'Sad'
    else:
        return 'Neutral'

def process_image(image_path):
    """Process a single image and analyze sentiments."""
    # Load image
    image = cv2.imread(image_path)
    if image is None:
        print("Error: Could not load image.")
        return

    # Detect faces
    faces = detect_faces(image)
    individual_sentiments = []

    for i, face in enumerate(faces):
        # Draw bounding box around the face
        x, y, w, h = face
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

        # Analyze expression
        sentiment = analyze_expression(face)
        individual_sentiments.append(sentiment)

        # Put sentiment text on the image
        cv2.putText(image, sentiment, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # Output the sentiment of each individual
    print(f"Image: {image_path}")
    for idx, sentiment in enumerate(individual_sentiments, start=1):
        print(f"Person {idx}: {sentiment}")

    # Count overall sentiment
    if individual_sentiments:
        sentiment_counts = {'Happy': 0, 'Sad': 0, 'Neutral': 0}
        for sentiment in individual_sentiments:
            sentiment_counts[sentiment] += 1
        overall_sentiment = max(sentiment_counts, key=sentiment_counts.get)
        print(f"Overall sentiment: {overall_sentiment}\n")
    else:
        print("No faces detected.\n")

    # Save the annotated image
    output_image_path = 'annotated_groupofpeople.jpg'
    success = cv2.imwrite(output_image_path, image)
    if success:
        print(f"Annotated image saved to {output_image_path}")
    else:
        print(f"Error saving annotated image to {output_image_path}")

    # Display the annotated image using cv2_imshow for Colab
    cv2_imshow(image)

# Example of changing the image input
image_path = '/content/people.jpeg'  # Change this to your image path
process_image(image_path)

import cv2
import numpy as np
from google.colab.patches import cv2_imshow  # Import cv2_imshow for displaying images in Colab

def detect_face(image):
    """Detect face in the image using Haar Cascades."""
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)
    return faces

def extract_features(face_region):
    """Extract geometric features from the face region."""
    height, width = face_region.shape[:2]
    jaw_width = width

    # Assuming two points for eyes (using simplistic approach)
    # In practice, more accurate landmarks would be used
    left_eye = (int(width * 0.3), int(height * 0.4))  # Placeholder
    right_eye = (int(width * 0.7), int(height * 0.4))  # Placeholder
    eye_distance = np.linalg.norm(np.array(left_eye) - np.array(right_eye))

    return jaw_width, eye_distance

def identify_gender(jaw_width, eye_distance):
    """Identify gender based on geometric features."""
    # Example thresholds
    if jaw_width > 100 and eye_distance < 50:
        return 'Male'
    else:
        return 'Female'

def process_image(image_path):
    """Process a single image and identify gender."""
    # Load image
    image = cv2.imread(image_path)
    if image is None:
        print("Error: Could not load image.")
        return

    # Detect face
    faces = detect_face(image)
    if len(faces) == 0:
        print("No faces detected.")
        return

    for (x, y, w, h) in faces:
        face_region = image[y:y+h, x:x+w]
        jaw_width, eye_distance = extract_features(face_region)
        gender = identify_gender(jaw_width, eye_distance)

        # Draw rectangle around the face and put the gender text
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(image, gender, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    print(f"Identified Gender: {gender}")

    # Display the annotated image using cv2_imshow
    cv2_imshow(image)

# Example of changing the image input
image_path = '/content/female.jpeg'  # Change this to your image path
process_image(image_path)